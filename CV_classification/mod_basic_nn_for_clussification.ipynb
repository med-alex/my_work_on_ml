{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 21154,
          "databundleVersionId": 1243559,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30628,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "4task_MedovikovAA.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'tpu-getting-started:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F21154%2F1243559%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240207%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240207T130447Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D589abaeb64d60d135b1d18072cf886233a2e511791eddbcc6c7db90f2f7701deaf77a53037ed85a79b3ab93833d4c28c9ecca22ebc1f5f6de6d3c6ff1b710897d64c9d97dfbff98689b107f57fd5c603dd7c83fc3a480879079014b66e0e3199f7c4bb019a0a30328972d610081c6c3ced1f2ae61184c63fc7da64b7900d6cfa17d4cb783e079c22f4a9ddfacc357badc82e279ce568f754641c82e32411f16e4b8f32d8638aa5d1b1e48f229d8c2ba27ea3587db3083a69ff6fbf573970646dca1c0585bf9777dc409ac68338bd166f3b312f2ef14c3fa92172fa222357c53d314e6f6bc184971326c914225e4fd7bdbf4e451f30683ef0d99e5b1b5199a97a'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "RGDO-ozMq8WR"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка"
      ],
      "metadata": {
        "id": "bMqwx39tq8WV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорты"
      ],
      "metadata": {
        "id": "oQZ6_yLjq8WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "# последовательная модель (стек слоев)\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "# полносвязный слой и слой выпрямляющий матрицу в вектор\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "# слой выключения нейронов и слой нормализации выходных данных (нормализует данные в пределах текущей выборки)\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, SpatialDropout2D, GaussianDropout\n",
        "# слои свертки и подвыборки\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "# работа с обратной связью от обучающейся нейронной сети\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# вспомогательные инструменты\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.regularizers import *\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.random import set_seed\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "seed = 42\n",
        "seed_everything(seed)\n",
        "\n",
        "# работа с изображениями\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#  библиотека для работы с наборами данных на Kaggle\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-01-05T12:32:26.266922Z",
          "iopub.execute_input": "2024-01-05T12:32:26.267279Z",
          "iopub.status.idle": "2024-01-05T12:32:29.928921Z",
          "shell.execute_reply.started": "2024-01-05T12:32:26.267248Z",
          "shell.execute_reply": "2024-01-05T12:32:29.928197Z"
        },
        "trusted": true,
        "id": "QJy00KQQq8WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "# Обнаружение оборудования, возврат соответствующей стратегии распространения: TPU, GPU, CPU\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Обнаружение TPU. Параметры среды не требуются, если задана переменная среды TPU_NAME. На Kaggle это всегда так.\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # стратегия распространения по умолчанию в Tensorflow. Работает на CPU и одном GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T12:32:29.930125Z",
          "iopub.execute_input": "2024-01-05T12:32:29.930537Z",
          "iopub.status.idle": "2024-01-05T12:32:37.60881Z",
          "shell.execute_reply.started": "2024-01-05T12:32:29.930509Z",
          "shell.execute_reply": "2024-01-05T12:32:37.607802Z"
        },
        "trusted": true,
        "id": "5woIKJ7sq8WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Данные"
      ],
      "metadata": {
        "id": "Oys3HskTq8Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
        "GCS_DS_PATH#получаем путь к наборам данных"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T12:32:37.609993Z",
          "iopub.execute_input": "2024-01-05T12:32:37.610249Z",
          "iopub.status.idle": "2024-01-05T12:32:37.616831Z",
          "shell.execute_reply.started": "2024-01-05T12:32:37.610222Z",
          "shell.execute_reply": "2024-01-05T12:32:37.616092Z"
        },
        "trusted": true,
        "id": "w1i8Z4BEq8Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = [192, 192] # при таком размере графическому процессору не хватит памяти. Используйте TPU\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "\n",
        "NUM_TRAINING_IMAGES = 12753\n",
        "NUM_TEST_IMAGES = 7382\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE # находим количество шагов за эпоху\n",
        "\n",
        "SEED = 2020"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T13:29:53.272207Z",
          "iopub.execute_input": "2024-01-05T13:29:53.272678Z",
          "iopub.status.idle": "2024-01-05T13:29:53.277746Z",
          "shell.execute_reply.started": "2024-01-05T13:29:53.272638Z",
          "shell.execute_reply": "2024-01-05T13:29:53.276764Z"
        },
        "trusted": true,
        "id": "r2GWMeH5q8Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции"
      ],
      "metadata": {
        "id": "DWM8F0twq8Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_image(image_data):\n",
        "    \"\"\"Декодирует изображение в vyjujvthye. vfnhbwe (тензор)\n",
        "    Нормализует данные и преобразовывает изображения к указанному размеру\"\"\"\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3) # Декодирование изображения в формате JPEG в тензор uint8.\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # преобразовать изображение в плавающее в диапазоне [0, 1]\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # явный размер, необходимый для TPU\n",
        "#     image = tf.keras.applications.inception_resnet_v2.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string означает байтовую строку\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # [] означает отдельный элемент\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT) # парсим отдельный пример в указанном формате\n",
        "    image = decode_image(example['image']) # преобразуем изображение к нужному нам формату\n",
        "    label = tf.cast(example['class'], tf.int32)\n",
        "    return image, label # возвращает набор данных пар (изображение, метка)\n",
        "\n",
        "def read_unlabeled_tfrecord(example):\n",
        "    UNLABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string означает байтовую строку\n",
        "        \"id\": tf.io.FixedLenFeature([], tf.string),  # [] означает отдельный элемент\n",
        "        # класс отсутствует, задача этого конкурса - предсказать классы цветов для тестового набора данных\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['image']) # преобразуем изображение к нужному нам формату\n",
        "    idnum = example['id']\n",
        "    return image, idnum # returns a dataset of image(s)\n",
        "\n",
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    \"\"\"Читает из TFRecords. Для оптимальной производительности одновременное чтение из нескольких\n",
        "    файлов без учета порядка данных. Порядок не имеет значения, поскольку мы все равно будем перетасовывать данные\"\"\"\n",
        "\n",
        "    ignore_order = tf.data.Options() # Представляет параметры для tf.data.Dataset.\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # отключить порядок, увеличить скорость\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames) # автоматически чередует чтение из нескольких файлов\n",
        "    dataset = dataset.with_options(ignore_order) # использует данные сразу после их поступления, а не в исходном порядке\n",
        "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord)\n",
        "    # возвращает набор данных пар (изображение, метка), если метка = Истина, или пар (изображение, идентификатор), если метка = Ложь\n",
        "    return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "    dataset = load_dataset(tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-192x192/train/*.tfrec'), labeled=True)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.repeat() # набор обучающих данных должен повторяться в течение нескольких эпох\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset():\n",
        "    dataset = load_dataset(tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-192x192/val/*.tfrec'), labeled=True, ordered=False)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.cache() # кешируем набор\n",
        "    return dataset\n",
        "\n",
        "def get_test_dataset(ordered=False):\n",
        "    dataset = load_dataset(tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-192x192/test/*.tfrec'), labeled=False, ordered=ordered)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    return dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T12:32:37.630303Z",
          "iopub.execute_input": "2024-01-05T12:32:37.630546Z",
          "iopub.status.idle": "2024-01-05T12:32:37.643129Z",
          "shell.execute_reply.started": "2024-01-05T12:32:37.630522Z",
          "shell.execute_reply": "2024-01-05T12:32:37.642406Z"
        },
        "trusted": true,
        "id": "otef6e5cq8Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Добавим аугументацию"
      ],
      "metadata": {
        "id": "pTytkcP7q8We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_augment(image, label):\n",
        "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
        "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
        "    # of the TPU while the TPU itself is computing gradients.\n",
        "    flag = random.randint(1,3)\n",
        "    # coef_1 = random.randint(70, 90) * 0.01\n",
        "    # coef_2 = random.randint(70, 90) * 0.01\n",
        "    if flag == 1:\n",
        "        image = tf.image.random_flip_left_right(image, seed=SEED)\n",
        "    elif flag == 2:\n",
        "        image = tf.image.random_flip_up_down(image, seed=SEED)\n",
        "    # else:\n",
        "        # image = tf.image.random_crop(image, [int(IMAGE_SIZE[0]*coef_1), int(IMAGE_SIZE[0]*coef_2), 3],seed=SEED, )\n",
        "    return image, label"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T12:32:37.644102Z",
          "iopub.execute_input": "2024-01-05T12:32:37.644644Z",
          "iopub.status.idle": "2024-01-05T12:32:37.657455Z",
          "shell.execute_reply.started": "2024-01-05T12:32:37.644615Z",
          "shell.execute_reply": "2024-01-05T12:32:37.656528Z"
        },
        "trusted": true,
        "id": "7uXN5mWKq8We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Печать резлуьтатов"
      ],
      "metadata": {
        "id": "2XqUhN6Jq8Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_score(history):\n",
        "\n",
        "  plt.plot(history.history['sparse_categorical_accuracy'],\n",
        "          label='Оценка точности на обучающем наборе')\n",
        "  plt.plot(history.history['val_sparse_categorical_accuracy'],\n",
        "          label='Оценка точности на проверочном наборе')\n",
        "  plt.xlabel('Эпоха обучения')\n",
        "  plt.ylabel('Оценка точности')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def print_loss(history):\n",
        "\n",
        "  plt.plot(history.history['loss'],\n",
        "         label='Оценка потерь на обучающем наборе')\n",
        "  plt.plot(history.history['val_loss'],\n",
        "          label='Оценка потерь на проверочном наборе')\n",
        "  plt.xlabel('Эпоха обучения')\n",
        "  plt.ylabel('Оценка потерь')\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T12:32:37.658508Z",
          "iopub.execute_input": "2024-01-05T12:32:37.65879Z",
          "iopub.status.idle": "2024-01-05T12:32:37.670784Z",
          "shell.execute_reply.started": "2024-01-05T12:32:37.658763Z",
          "shell.execute_reply": "2024-01-05T12:32:37.670057Z"
        },
        "trusted": true,
        "id": "tgosqbquq8Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Датасеты"
      ],
      "metadata": {
        "id": "jjFooWBRq8Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = get_training_dataset()\n",
        "validation_dataset = get_validation_dataset()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T12:32:37.671733Z",
          "iopub.execute_input": "2024-01-05T12:32:37.671979Z",
          "iopub.status.idle": "2024-01-05T12:32:38.248698Z",
          "shell.execute_reply.started": "2024-01-05T12:32:37.671953Z",
          "shell.execute_reply": "2024-01-05T12:32:38.247624Z"
        },
        "trusted": true,
        "id": "ywnr0LOAq8Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модель"
      ],
      "metadata": {
        "id": "b7jhdavEq8Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_model():\n",
        "\n",
        "#     # Создаем последовательную модель\n",
        "#     model = Sequential()\n",
        "\n",
        "#     ## Первый сверточный блок\n",
        "#     # Первый сверточный слой\n",
        "#     model.add(Conv2D(64, (5, 5), input_shape=(*IMAGE_SIZE, 3), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Второй сверточный слой\n",
        "#     model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # # Первый слой подвыборки\n",
        "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     # Первый Слой регуляризации Dropout\n",
        "#     model.add(GaussianDropout(0.25))\n",
        "\n",
        "#     ## Второй сверточный блок\n",
        "#     # Четвертый сверточный слой\n",
        "#     model.add(Conv2D(128, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Пятый сверточный слой\n",
        "#     model.add(Conv2D(128, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Второй слой подвыборки\n",
        "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     # Второй Слой регуляризации Dropout\n",
        "#     model.add(GaussianDropout(0.35))\n",
        "\n",
        "#     ## Третий сверточный блок\n",
        "#     # Шестой сверточный слой\n",
        "#     model.add(Conv2D(256, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Седьмой сверточный слой\n",
        "#     model.add(Conv2D(256, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Восьмой сверточный слой\n",
        "#     model.add(Conv2D(256, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Третий слой подвыборки\n",
        "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     # Третий Слой регуляризации Dropout\n",
        "#     model.add(GaussianDropout(0.45))\n",
        "\n",
        "#     ## Четвертый сверточный блок\n",
        "#     # Девятый сверточный слой\n",
        "#     model.add(Conv2D(512, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Десятый сверточный слой\n",
        "#     model.add(Conv2D(512, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Одиннадцатый сверточный слой\n",
        "#     model.add(Conv2D(512, (5, 5), activation='relu'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Четвертый слой подвыборки\n",
        "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     # Четвертый Слой регуляризации Dropout\n",
        "#     model.add(GaussianDropout(0.5))\n",
        "\n",
        "#     ## Полносвязный блок\n",
        "#     # Слой преобразования данных из 2D представления в плоское\n",
        "#     model.add(Flatten())\n",
        "#     # Полносвязный слой для классификации\n",
        "#     model.add(Dense(1024, activation='relu'))\n",
        "#     # Четвертый слой нормализации данных\n",
        "#     model.add(BatchNormalization())\n",
        "#     # Четвертый Слой регуляризации Dropout\n",
        "#     model.add(GaussianDropout(0.8))\n",
        "#     # Выходной полносвязный слой\n",
        "#     model.add(Dense(104, activation='softmax'))\n",
        "#     return model\n",
        "\n",
        "\n",
        "# with strategy.scope():\n",
        "#     model = get_model()\n",
        "# model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dRbXNjqNq8Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    # Создаем новую последовательную модель\n",
        "    model = Sequential()\n",
        "\n",
        "    # Первый сверточный блок\n",
        "    model.add(Conv2D(128, (2, 2), input_shape=(*IMAGE_SIZE, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128, (2, 2), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling2D(pool_size=(5, 5)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # Второй сверточный блок\n",
        "    model.add(Conv2D(256, (2, 2), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(256, (2, 2), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling2D(pool_size=(3, 3)))\n",
        "    model.add(Dropout(0.35))\n",
        "\n",
        "    # Третий сверточный блок\n",
        "    model.add(Conv2D(512, (2, 2), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(512, (2, 2), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(512, (2, 2), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling2D(pool_size=(3, 3)))\n",
        "    model.add(Dropout(0.45))\n",
        "\n",
        "    # Полносвязный блок\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(104, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "    model = get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T13:30:06.494766Z",
          "iopub.execute_input": "2024-01-05T13:30:06.49514Z",
          "iopub.status.idle": "2024-01-05T13:30:07.467104Z",
          "shell.execute_reply.started": "2024-01-05T13:30:06.49511Z",
          "shell.execute_reply": "2024-01-05T13:30:07.466226Z"
        },
        "trusted": true,
        "id": "2fiohD2zq8Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks_list = [EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3),\n",
        "                  ]\n",
        "\n",
        "model.compile(\n",
        "    optimizer='nadam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics=['sparse_categorical_accuracy']\n",
        ")\n",
        "\n",
        "historical = model.fit(training_dataset,\n",
        "          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "          epochs=EPOCHS,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=validation_dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T13:30:10.315638Z",
          "iopub.execute_input": "2024-01-05T13:30:10.315958Z",
          "iopub.status.idle": "2024-01-05T13:34:01.626572Z",
          "shell.execute_reply.started": "2024-01-05T13:30:10.31593Z",
          "shell.execute_reply": "2024-01-05T13:34:01.62534Z"
        },
        "trusted": true,
        "id": "tWZQnpXQq8Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_score(historical)\n",
        "print_loss(historical)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T13:34:05.365961Z",
          "iopub.execute_input": "2024-01-05T13:34:05.366365Z",
          "iopub.status.idle": "2024-01-05T13:34:05.645287Z",
          "shell.execute_reply.started": "2024-01-05T13:34:05.366334Z",
          "shell.execute_reply": "2024-01-05T13:34:05.644289Z"
        },
        "trusted": true,
        "id": "RvQ-LD0Eq8Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тест на submission"
      ],
      "metadata": {
        "id": "jcS1f7C5q8Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = get_test_dataset(ordered=True)\n",
        "\n",
        "print('Вычисляем предсказания...')\n",
        "test_images_ds = test_ds.map(lambda image, idnum: image)\n",
        "probabilities = model.predict(test_images_ds)\n",
        "predictions = np.argmax(probabilities, axis=-1)\n",
        "print(predictions)\n",
        "\n",
        "print('Создание файла submission.csv...')\n",
        "test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
        "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # все в одной партии\n",
        "np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-05T13:34:13.019408Z",
          "iopub.execute_input": "2024-01-05T13:34:13.019911Z",
          "iopub.status.idle": "2024-01-05T13:34:21.51614Z",
          "shell.execute_reply.started": "2024-01-05T13:34:13.019858Z",
          "shell.execute_reply": "2024-01-05T13:34:21.515099Z"
        },
        "trusted": true,
        "id": "rFGB27EDq8Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:8af5ec8c-8370-4db3-ba4a-c27ae41b44e4.png)"
      ],
      "metadata": {
        "id": "Is8YcX73q8Wj"
      }
    }
  ]
}